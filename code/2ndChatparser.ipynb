{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f79ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:38:26,084 - INFO - Report written to ../data\\excel\\sas_migration_report2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants for write-back detection (permanent libs)\n",
    "PERMANENT_LIBS = [r\"^prod\\.\", r\"^finance\\.\", r\"^analytics\\.\"]\n",
    "\n",
    "\n",
    "def read_sas_files(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively read all .sas files in the given folder.\n",
    "    Returns a dict: {file_path: file_content}\n",
    "    \"\"\"\n",
    "    sas_files = glob.glob(os.path.join(folder_path, '**', '*.sas'), recursive=True)\n",
    "    files = {}\n",
    "    for fp in sas_files:\n",
    "        try:\n",
    "            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                files[fp] = f.read()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to read {fp}: {e}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def strip_comments(code):\n",
    "    \"\"\"\n",
    "    Remove SAS comments (*...; and /*...*/), preserving inline code.\n",
    "    \"\"\"\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL | re.IGNORECASE)\n",
    "    code = re.sub(r'\\*[^;]*;', '', code, flags=re.IGNORECASE)\n",
    "    return code\n",
    "\n",
    "\n",
    "def find_libnames(code):\n",
    "    \"\"\"\n",
    "    Extract declared librefs from LIBNAME statements.\n",
    "    Returns set of libref names.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"\\bLIBNAME\\s+(?P<lib>\\w+)\\s*=\", re.IGNORECASE)\n",
    "    return set(m.group('lib').lower() for m in pattern.finditer(code))\n",
    "\n",
    "\n",
    "def find_includes(code, file_dir):\n",
    "    \"\"\"\n",
    "    Extract %INCLUDE paths (single/double) and macro-based paths.\n",
    "    Returns list of tuples (path, exists_boolean).\n",
    "    \"\"\"\n",
    "    includes = []\n",
    "    # match single or double quoted\n",
    "    pattern = re.compile(r\"%INCLUDE\\s+(?:'|\\\")(?P<path>[^'\\\"]+)(?:'|\\\")\", re.IGNORECASE)\n",
    "    for m in pattern.finditer(code):\n",
    "        path = m.group('path')\n",
    "        full = os.path.join(file_dir, path)\n",
    "        includes.append((path, os.path.exists(full)))\n",
    "    return includes\n",
    "\n",
    "\n",
    "def detect_blocks(code):\n",
    "    \"\"\"\n",
    "    Identify and isolate PROC, DATA, MACRO blocks with specific end markers.\n",
    "    Returns list of dicts with keys: type, name, raw_code.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "\n",
    "    # 1. MACRO definitions\n",
    "    for m in re.finditer(r\"%MACRO\\s+(?P<name>\\w+)(?P<params>\\([^)]*\\))?;\", code, flags=re.IGNORECASE):\n",
    "        start = m.start()\n",
    "        # find corresponding %MEND\n",
    "        end_match = re.search(rf\"%MEND\\s+{re.escape(m.group('name'))};\", code[m.end():], flags=re.IGNORECASE)\n",
    "        end = m.end() + (end_match.end() if end_match else 0)\n",
    "        raw = code[start:end]\n",
    "        blocks.append({'type': 'MACRO', 'name': m.group('name'), 'raw': raw})\n",
    "\n",
    "    # 2. DATA steps\n",
    "    for m in re.finditer(r\"\\bDATA\\s+(?P<name>[\\w\\.]+)\\s*;\", code, flags=re.IGNORECASE):\n",
    "        start = m.start()\n",
    "        # DATA steps end with RUN;\n",
    "        end_match = re.search(r\"\\bRUN;\", code[m.end():], flags=re.IGNORECASE)\n",
    "        end = m.end() + (end_match.end() if end_match else 0)\n",
    "        raw = code[start:end]\n",
    "        blocks.append({'type': 'DATA', 'name': m.group('name'), 'raw': raw})\n",
    "\n",
    "    # 3. PROC steps\n",
    "    proc_pattern = re.compile(r\"\\bPROC\\s+(?P<name>\\w+)\\b\", flags=re.IGNORECASE)\n",
    "    for m in proc_pattern.finditer(code):\n",
    "        start = m.start()\n",
    "        name = m.group('name').upper()\n",
    "        rest = code[m.end():]\n",
    "        # interpret end based on PROC\n",
    "        if name == 'SQL':\n",
    "            end_match = re.search(r\"\\bQUIT;\", rest, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            end_match = re.search(r\"\\bRUN;\", rest, flags=re.IGNORECASE)\n",
    "        end = m.end() + (end_match.end() if end_match else 0)\n",
    "        raw = code[start:end]\n",
    "        blocks.append({'type': 'PROC', 'name': name, 'raw': raw})\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def extract_tables_for_block(block):\n",
    "    \"\"\"\n",
    "    Extract input and output tables depending on block type with tailored patterns.\n",
    "    Returns two sets: inputs, outputs.\n",
    "    \"\"\"\n",
    "    raw = block['raw']\n",
    "    inputs, outputs = set(), set()\n",
    "\n",
    "    if block['type'] == 'DATA':\n",
    "        # SET, MERGE, IN=, etc.\n",
    "        for pat in [r\"\\bSET\\s+(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bMERGE\\s+(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bIN=\\s*(?P<table>[\\w\\.]+)\"]:\n",
    "            for m in re.finditer(pat, raw, flags=re.IGNORECASE):\n",
    "                inputs.add(m.group('table').lower())\n",
    "        # OUTPUT dataset for DATA step is the name after DATA statement\n",
    "        data_name = re.search(r\"\\bDATA\\s+(?P<name>[\\w\\.]+)\\s*;\", raw, flags=re.IGNORECASE)\n",
    "        if data_name:\n",
    "            outputs.add(data_name.group('name').lower())\n",
    "\n",
    "    elif block['type'] == 'PROC' and block['name'] == 'SQL':\n",
    "        # PROC SQL: FROM, JOIN, INTO, CREATE TABLE\n",
    "        for pat in [r\"\\bFROM\\s+(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bJOIN\\s+(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bINTO\\s+(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bCREATE\\s+TABLE\\s+(?P<table>[\\w\\.]+)\"]:\n",
    "            for m in re.finditer(pat, raw, flags=re.IGNORECASE):\n",
    "                if pat.startswith(\"FROM\") or pat.startswith(\"JOIN\"):\n",
    "                    inputs.add(m.group('table').lower())\n",
    "                else:\n",
    "                    outputs.add(m.group('table').lower())\n",
    "\n",
    "    else:\n",
    "        # Generic PROC: look for OUT=, DATA= for output, any TABLE= for input if needed\n",
    "        for pat in [r\"\\bOUT=\\s*(?P<table>[\\w\\.]+)\",\n",
    "                    r\"\\bDATA=\\s*(?P<table>[\\w\\.]+)\"]:\n",
    "            for m in re.finditer(pat, raw, flags=re.IGNORECASE):\n",
    "                outputs.add(m.group('table').lower())\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def detect_write_back(tables):\n",
    "    \"\"\"\n",
    "    Returns True if any table write references a permanent lib.\n",
    "    \"\"\"\n",
    "    for tbl in tables:\n",
    "        for lib in PERMANENT_LIBS:\n",
    "            if re.match(lib, tbl, flags=re.IGNORECASE):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def comment_write_backs(raw_code):\n",
    "    \"\"\"\n",
    "    Comment out lines referencing permanent libraries.\n",
    "    \"\"\"\n",
    "    lines = raw_code.splitlines()\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        if any(re.search(lib, ln, flags=re.IGNORECASE) for lib in PERMANENT_LIBS):\n",
    "            out.append(f\"* {ln}  /* commented out by migration tool */\")\n",
    "        else:\n",
    "            out.append(ln)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def parse_file(path, content):\n",
    "    \"\"\"\n",
    "    Parse a single SAS file and return list of metadata dicts for each block.\n",
    "    \"\"\"\n",
    "    clean = strip_comments(content)\n",
    "    librefs = find_libnames(clean)\n",
    "    incs = find_includes(clean, os.path.dirname(path))\n",
    "    blocks = detect_blocks(clean)\n",
    "    records = []\n",
    "\n",
    "    for b in blocks:\n",
    "        try:\n",
    "            inp, outp = extract_tables_for_block(b)\n",
    "            write_flag = 'Yes' if detect_write_back(outp | inp) else 'No'\n",
    "            missing_libs = [tbl.split('.')[0] for tbl in (inp | outp)\n",
    "                            if tbl.split('.')[0] not in librefs and tbl.split('.')[0] != 'work']\n",
    "            missing_includes = 'Yes' if any(not ex for _, ex in incs) else 'No'\n",
    "            raw_code = comment_write_backs(b['raw']) if write_flag == 'Yes' else b['raw']\n",
    "\n",
    "            records.append({\n",
    "                'file_name': os.path.basename(path),\n",
    "                'block_type': b['type'],\n",
    "                'block_name': b.get('name'),\n",
    "                'input_tables': list(inp) or None,\n",
    "                'output_tables': list(outp) or None,\n",
    "                'write_back': write_flag,\n",
    "                'missing_librefs': missing_libs or None,\n",
    "                'missing_includes': missing_includes,\n",
    "                'raw_code': raw_code\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing block {b.get('name')} in {path}: {e}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def main(folder):\n",
    "    files = read_sas_files(folder)\n",
    "    all_records = []\n",
    "    for path, content in files.items():\n",
    "        all_records.extend(parse_file(path, content))\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    out_folder = os.path.join(folder, 'excel')\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    out_path = os.path.join(out_folder, 'sas_migration_report2.xlsx')\n",
    "    df.to_excel(out_path, index=False)\n",
    "    logging.info(f\"Report written to {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    folder = \"../data\" \n",
    "    main(folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
